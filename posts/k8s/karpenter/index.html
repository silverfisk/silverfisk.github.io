<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Tom"><meta name=description content="https://linux-konsult.com"><meta name=keywords content="blog,developer,esp8266,prometheus,grafana,bitcoin"><meta property="og:site_name" content="linux-konsult.com"><meta property="og:title" content="
  Let karpenter just-in-time scheduler manage disk pressure - linux-konsult.com
"><meta property="og:description" content="Let karpenter just-in-time scheduler manage disk pressure"><meta property="og:type" content="website"><meta property="og:url" content="https://linux-konsult.com/posts/k8s/karpenter/"><meta property="og:image" content="https://linux-konsult.com/img/karpenter-logo.png"><meta name=twitter:card content="summary"><meta name=twitter:site content="https://linux-konsult.com/posts/k8s/karpenter/"><meta name=twitter:image content="https://linux-konsult.com/img/karpenter-logo.png"><base href=https://linux-konsult.com/posts/k8s/karpenter/><title>Let karpenter just-in-time scheduler manage disk pressure - linux-konsult.com</title><link rel=canonical href=https://linux-konsult.com/posts/k8s/karpenter/><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.2.0/css/all.css integrity=sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ crossorigin=anonymous><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700"><link rel=stylesheet href=/css/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=alternate href=https://linux-konsult.com/index.xml type=application/rss+xml title=linux-konsult.com><link href=https://linux-konsult.com/index.xml rel=feed type=application/rss+xml title=linux-konsult.com><meta name=generator content="Hugo 0.104.3"></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>linux-konsult.com</a>
<input type=checkbox id=menu-control>
<label class="menu-mobile float-right" for=menu-control><span class="btn-mobile float-right">&#9776;</span><ul class=navigation-list><li class="navigation-item align-center"><a class=navigation-link href=https://linux-konsult.com/projects>Projects</a></li><li class="navigation-item align-center"><a class=navigation-link href=https://linux-konsult.com/posts>Blog</a></li><li class="navigation-item align-center"><a class=navigation-link href=https://linux-konsult.com/links>Links</a></li><li class="navigation-item align-center"><a class=navigation-link href=https://whoami.linux-konsult.com>Resume</a></li></ul></label></section></nav><div class=content><section class="container post"><article><header><h1 class=title>Let karpenter just-in-time scheduler manage disk pressure</h1><h2 class=date>October 27, 2022</h2></header><h2 id=overview>Overview</h2><hr><p>Karpenter is used for automatic rightscaling (and rightsizing) kubernetes nodes in a cluster.</p><p>For this the <code>Provisioner</code> kind is used to configure the just-in-time node scheduler.</p><p>An issue we have is that GitLab CI jobs fill up node storage. GitLab creates a new pod for each CI job.</p><p>This lets karpenter configure the nodes to automatically perform garbage collection, and taint itself to avoid further pod scheduling. As a last resort it will kill greedy CI jobs to keep the cluster as a whole more consistent and performant.</p><h3 id=karpenter-kubelet-configuration>Karpenter kubelet Configuration</h3><hr><p>The idea here is to let Karpenter set a kubeletConfiguration, so the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/>kubelet</a> starts the <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a> process to proactively terminate pods to reclaim resources on node.</p><p>This may fail some GitLab CI jobs when node disks are full, but it also reduce the blast radius when a job fill up disks for other jobs.</p><p>Check if the cluster already has a kubeletConfiguration:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=o>[</span>localhost<span class=o>]</span>$ kubectl get provisioners.karpenter.sh default  -oyaml <span class=p>|</span> yq .spec.kubeletConfiguration
</span></span></code></pre></div><hr><h2 id=configuration-knobs>Configuration knobs</h2><p>Setting these values may actually kill some pods and their CI jobs as a last resort. In our setup the most common issue of full disks are when very CI jobs are executed in very big container images.</p><p>Before any jobs are evicted because of DiskPressure, the kubelet will try to reclaim resources by culling dead pods and images.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>kubeletConfiguration</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>ephemeral-storage</span><span class=p>:</span><span class=w> </span><span class=l>15Gi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>kubeReserved</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=l>200m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class=l>100Mi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>ephemeral-storage</span><span class=p>:</span><span class=w> </span><span class=l>3Gi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>evictionHard</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>memory.available</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>nodefs.available</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>nodefs.inodesFree</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>evictionSoft</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>memory.available</span><span class=p>:</span><span class=w> </span><span class=l>500Mi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>nodefs.available</span><span class=p>:</span><span class=w> </span><span class=m>15</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>nodefs.inodesFree</span><span class=p>:</span><span class=w> </span><span class=m>15</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>evictionSoftGracePeriod</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>memory.available</span><span class=p>:</span><span class=w> </span><span class=l>3m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>nodefs.available</span><span class=p>:</span><span class=w> </span><span class=l>1m30s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>nodefs.inodesFree</span><span class=p>:</span><span class=w> </span><span class=l>2m</span><span class=w>
</span></span></span></code></pre></div><hr><p>Setting this would also taint the node with <code>node.kubernetes.io/disk-pressure</code>. Given this, the node is tainted so no new CI pods is started, a new instance spins up with enough disk storage to acceppt the new jobs.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>taints</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>key</span><span class=p>:</span><span class=w> </span><span class=l>node.kubernetes.io/disk-pressure</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>effect</span><span class=p>:</span><span class=w> </span><span class=l>NoSchedule</span><span class=w>
</span></span></span></code></pre></div></article><br></section></div><footer class=footer><section class=container><div class="sns-shares sp-sns-shares"></div></section></footer><div class=fixed-bar><section class=container><div class="sns-shares pc-sns-shares"></div></section></div></main><script src=/js/app.js></script></body></html>