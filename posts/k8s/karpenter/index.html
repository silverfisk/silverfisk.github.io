<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Tom"><meta name=description content="https://linux-konsult.com"><meta name=keywords content="blog,devops,sustainability,esp8266,prometheus,grafana"><meta property="og:site_name" content="linux-konsult.com"><meta property="og:title" content="
  Let karpenters just-in-time scheduler manage disk pressure - linux-konsult.com
"><meta property="og:description" content="Kapacity planning in kubernetes"><meta property="og:type" content="website"><meta property="og:url" content="https://linux-konsult.com/posts/k8s/karpenter/"><meta property="og:image" content="https://linux-konsult.com/img/karpenter-logo.png"><meta name=twitter:card content="summary"><meta name=twitter:site content="https://linux-konsult.com/posts/k8s/karpenter/"><meta name=twitter:image content="https://linux-konsult.com/img/karpenter-logo.png"><base href=https://linux-konsult.com/posts/k8s/karpenter/><title>Let karpenters just-in-time scheduler manage disk pressure - linux-konsult.com</title><link rel=canonical href=https://linux-konsult.com/posts/k8s/karpenter/><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.2.0/css/all.css integrity=sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ crossorigin=anonymous><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700"><link rel=stylesheet href=/css/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=alternate href=https://linux-konsult.com/index.xml type=application/rss+xml title=linux-konsult.com><link href=https://linux-konsult.com/index.xml rel=feed type=application/rss+xml title=linux-konsult.com><meta name=generator content="Hugo 0.108.0"></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>linux-konsult.com</a>
<input type=checkbox id=menu-control>
<label class="menu-mobile float-right" for=menu-control><span class="btn-mobile float-right">&#9776;</span><ul class=navigation-list><li class="navigation-item align-center"><a class=navigation-link href=https://linux-konsult.com/posts>Blog</a></li><li class="navigation-item align-center"><a class=navigation-link href=https://linux-konsult.com/projects>Projects</a></li><li class="navigation-item align-center"><a class=navigation-link href=https://linux-konsult.com/links>Links</a></li><li class="navigation-item align-center"><a class=navigation-link href=https://whoami.linux-konsult.com>Resume</a></li></ul></label></section></nav><div class=content><section class="container post"><article><header><h1 class=title>Let karpenters just-in-time scheduler manage disk pressure</h1><h2 class=date>October 27, 2022</h2></header><h2 id=overview>Overview</h2><hr><p>Capacity planning in Kubernetes can be challenging.
Karpenter is useful for automatically rightscaling and rightsizing Kubernetes nodes in an EKS cluster, but it usually only considers CPU and memory usage. We experienced difficulties troubleshooting this issue because the node storage was not full at the time we checked it. The disk usage would spike to almost capacity (98-99%) during the execution of GitLab CI jobs, then decrease after the job was completed. This made it difficult to identify the cause of the issue.</p><p>To address this issue, we used the<code>Provisioner</code> kind to configure the just-in-time node scheduler to place bids for new (or larger) spot instance nodes. In addition to providing a better ROI on the cluster, this approach also helps to improve Cloud Sustainability targets, such as <a href=https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sus_sus_hardware_a2.html>SUS05-BP01</a>, by using the minimum amount of hardware needed to meet your needs if you follow the new <a href=https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html>AWS Well Architected Sustaninability Pillar</a>.</p><p>This lets karpenter configure nodes to automatically perform garbage collection, and taint itself to avoid further pod scheduling. As a last resort it will kill greedy CI jobs to keep the cluster as a whole more consistent, performant and stable.<div class=portfolio><div class=portfolio-inner><div class=portfolio-image><img src=/img/k8s_rightsizing.png alt></div><div class=portfolio-content><h3 id=karpenter-kubelet-configuration>Karpenter kubelet Configuration</h3><hr><p>One way to address the issue of full disks on Kubernetes nodes is to use Karpenter to set a kubeletConfiguration. This allows the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/>kubelet</a> to start the <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a> process, which proactively terminates pods to reclaim resources on the affected node.</p><p>While this approach may cause some GitLab CI jobs to fail when node disks are full, it also reduces the impact on other jobs by limiting the &ldquo;blast radius&rdquo; of a job that fills up disks. In general, it can help to keep the cluster more consistent, performant, and stable.</p><p>Check if the cluster already has a kubeletConfiguration:</p></div></div></div></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=o>[</span>localhost<span class=o>]</span>$ kubectl get provisioners.karpenter.sh default  -oyaml <span class=p>|</span> yq .spec.kubeletConfiguration
</span></span></code></pre></div><hr><h2 id=configuration-knobs>Configuration knobs</h2><p>Setting these values may cause some pods and their CI jobs to be killed as a last resort. In our setup, the most common issue with full disks was CI jobs executing in very large container images which caused the image storage to fill up. It is important to investigate whether adding more node disk space would be a more effective preventive solution, as this is more of a way to handle node scaling if the disks fill up despite preventive actions already being taken.</p><p>Before any jobs are evicted due to DiskPressure, the kubelet will attempt to reclaim resources by culling dead pods and images.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>karpenter.sh/v1alpha5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Provisioner</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=l>&lt;...&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>kubeletConfiguration</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>ephemeral-storage</span><span class=p>:</span><span class=w> </span><span class=l>15Gi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>kubeReserved</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=l>200m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class=l>100Mi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>ephemeral-storage</span><span class=p>:</span><span class=w> </span><span class=l>3Gi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>evictionHard</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory.available</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.available</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.inodesFree</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>evictionSoft</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory.available</span><span class=p>:</span><span class=w> </span><span class=l>500Mi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.available</span><span class=p>:</span><span class=w> </span><span class=m>15</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.inodesFree</span><span class=p>:</span><span class=w> </span><span class=m>15</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>evictionSoftGracePeriod</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory.available</span><span class=p>:</span><span class=w> </span><span class=l>3m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.available</span><span class=p>:</span><span class=w> </span><span class=l>1m30s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.inodesFree</span><span class=p>:</span><span class=w> </span><span class=l>2m</span><span class=w>
</span></span></span></code></pre></div></article><br></section></div><footer class=footer><section class=container><div class="sns-shares sp-sns-shares"></div></section></footer><div class=fixed-bar><section class=container><p id=privateTriggerText>Contact details <a id=privateTrigger>Click!</a></p><div class="sns-shares pc-sns-shares"></div></section></div></main><script src=/js/app.js></script>
<script>(function(e){e(function(){e("#privateTrigger").on("click",function(){e(".private").slideToggle(),e("#privateTriggerText").text("tom@linux-konsult.com")})})})(jQuery)</script></body></html>