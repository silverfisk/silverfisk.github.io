<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Tom"><meta name=description content="https://linux-konsult.com"><meta name=keywords content="blog,developer,esp8266,prometheus,grafana,bitcoin"><meta property="og:site_name" content="linux-konsult.com"><meta property="og:title" content="
  Let karpenters just-in-time scheduler manage disk pressure - linux-konsult.com
"><meta property="og:description" content="Kapacity planning in kubernetes"><meta property="og:type" content="website"><meta property="og:url" content="https://linux-konsult.com/posts/k8s/karpenter/"><meta property="og:image" content="https://linux-konsult.com/img/karpenter-logo.png"><meta name=twitter:card content="summary"><meta name=twitter:site content="https://linux-konsult.com/posts/k8s/karpenter/"><meta name=twitter:image content="https://linux-konsult.com/img/karpenter-logo.png"><base href=https://linux-konsult.com/posts/k8s/karpenter/><title>Let karpenters just-in-time scheduler manage disk pressure - linux-konsult.com</title><link rel=canonical href=https://linux-konsult.com/posts/k8s/karpenter/><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.2.0/css/all.css integrity=sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ crossorigin=anonymous><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700"><link rel=stylesheet href=/css/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=alternate href=https://linux-konsult.com/index.xml type=application/rss+xml title=linux-konsult.com><link href=https://linux-konsult.com/index.xml rel=feed type=application/rss+xml title=linux-konsult.com><meta name=generator content="Hugo 0.107.0"></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>linux-konsult.com</a>
<input type=checkbox id=menu-control>
<label class="menu-mobile float-right" for=menu-control><span class="btn-mobile float-right">&#9776;</span><ul class=navigation-list><li class="navigation-item align-center"><a class=navigation-link href=https://linux-konsult.com/projects>Projects</a></li><li class="navigation-item align-center"><a class=navigation-link href=https://linux-konsult.com/posts>Blog</a></li><li class="navigation-item align-center"><a class=navigation-link href=https://linux-konsult.com/links>Links</a></li><li class="navigation-item align-center"><a class=navigation-link href=https://whoami.linux-konsult.com>Resume</a></li></ul></label></section></nav><div class=content><section class="container post"><article><header><h1 class=title>Let karpenters just-in-time scheduler manage disk pressure</h1><h2 class=date>October 27, 2022</h2></header><h2 id=overview>Overview</h2><hr><p>It can be challenging to do capacity planning in kubernetes.
Karpenter can be used for automatic rightscaling (and rightsizing) of kubernetes nodes in an EKS cluster, but it usually only handle CPU and memory. We had an issue where GitLab CI jobs quickly filled up node storage. The node disk usage was big enough to almost fill the disk to 98-99%. The CI job itself did not fail, but parallell jobs sheduled on the same node sometimes would.</p><p>GitLab creates a new pod for each CI job.</p><p>For this the <code>Provisioner</code> kind is used to configure the just-in-time node scheduler to place bids for new (or bigger) spot instance nodes. In addition to the better ROI on the cluster, it also improve Cloud Sustainability targets such as <a href=https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sus_sus_hardware_a2.html>SUS05-BP01</a> to use the minimum amount of hardware to meet your needs if you follow the new <a href=https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html>AWS Well Architected Sustaninability Pillar</a>.</p><p>This lets karpenter configure node to automatically perform garbage collection, and taint itself to avoid further pod scheduling. As a last resort it will kill greedy CI jobs to keep the cluster as a whole more consistent, performant and stable.<div class=portfolio><div class=portfolio-inner><div class=portfolio-image><img src=/img/k8s_rightsizing.png alt></div><div class=portfolio-content><h3 id=karpenter-kubelet-configuration>Karpenter kubelet Configuration</h3><hr><p>The idea here is to let Karpenter set a kubeletConfiguration, so the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/>kubelet</a> starts the <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a> process to proactively terminate pods to reclaim resources on the affected node.</p><p>This may fail some GitLab CI jobs when node disks are full, but it also reduce the blast radius when a job fill up disks for other jobs.</p><p>Check if the cluster already has a kubeletConfiguration:</p></div></div></div></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=o>[</span>localhost<span class=o>]</span>$ kubectl get provisioners.karpenter.sh default  -oyaml <span class=p>|</span> yq .spec.kubeletConfiguration
</span></span></code></pre></div><hr><h2 id=configuration-knobs>Configuration knobs</h2><p>Setting these values may actually kill some pods and their CI jobs as a last resort. In our setup the most common issue of full disks are when very CI jobs are executed in very big container images. Naturally it should be investigated if more node disk space should be added as a preventive solution, since this is more a way to handle node scaling <em>if the disks fill up in spite of the preventive actions already taken</em>.</p><p>Before any jobs are evicted because of DiskPressure, the kubelet will try to reclaim resources by culling dead pods and images.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>karpenter.sh/v1alpha5</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Provisioner</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=l>&lt;...&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>kubeletConfiguration</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>ephemeral-storage</span><span class=p>:</span><span class=w> </span><span class=l>15Gi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>kubeReserved</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=l>200m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory</span><span class=p>:</span><span class=w> </span><span class=l>100Mi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>ephemeral-storage</span><span class=p>:</span><span class=w> </span><span class=l>3Gi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>evictionHard</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory.available</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.available</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.inodesFree</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>evictionSoft</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory.available</span><span class=p>:</span><span class=w> </span><span class=l>500Mi</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.available</span><span class=p>:</span><span class=w> </span><span class=m>15</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.inodesFree</span><span class=p>:</span><span class=w> </span><span class=m>15</span><span class=l>%</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>evictionSoftGracePeriod</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>memory.available</span><span class=p>:</span><span class=w> </span><span class=l>3m</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.available</span><span class=p>:</span><span class=w> </span><span class=l>1m30s</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nodefs.inodesFree</span><span class=p>:</span><span class=w> </span><span class=l>2m</span><span class=w>
</span></span></span></code></pre></div></article><br></section></div><footer class=footer><section class=container><div class="sns-shares sp-sns-shares"></div></section></footer><div class=fixed-bar><section class=container><div class="sns-shares pc-sns-shares"></div></section></div></main><script src=/js/app.js></script></body></html>